\documentclass[
  12pt,
  bibliography=totoc,     % Literatur im Inhaltsverzeichnis
  captions=tableheading,  % Tabellenüberschriften
  titlepage=firstiscover, % Titelseite ist Deckblatt
]{scrartcl}

% Paket float verbessern
\usepackage{scrhack}

% Warnung, falls nochmal kompiliert werden muss
\usepackage[aux]{rerunfilecheck}

% unverzichtbare Mathe-Befehle
\usepackage{amsmath}
% viele Mathe-Symbole
\usepackage{amssymb}
% Erweiterungen für amsmath
\usepackage{mathtools}

% Setzt die Seitenränder
\usepackage[a4paper, left=2.5cm, right=2.5cm, top=3.5cm, bottom=3.5cm]{geometry}

% Fonteinstellungen
\usepackage{fontspec}
% Latin Modern Fonts werden automatisch geladen
% Alternativ zum Beispiel:
%\setromanfont{Libertinus Serif}
%\setsansfont{Libertinus Sans}
% \setmonofont{Libertinus Mono}
% \setmainfont{Times New Roman}
\setmainfont{TeX Gyre Termes}
\addtokomafont{disposition}{\rmfamily} % Anpassen aller Überschriften

% Paket für Zeilenabstände
\usepackage{setspace}
\onehalfspacing % 1.5-facher Zeilenabstand gefordert

% Wenn man andere Schriftarten gesetzt hat,
% sollte man das Seiten-Layout neu berechnen lassen
\recalctypearea{}

% deutsche Spracheinstellungen
\usepackage[english]{babel}
%BUG in Biblatex wird hiermit gefixt
\providetoggle{blx@lang@captions@english}


\usepackage[
  math-style=ISO,    % ┐
  bold-style=ISO,    % │
  sans-style=italic, % │ ISO-Standard folgen
  nabla=upright,     % │
  partial=upright,   % ┘
  warnings-off={           % ┐
    mathtools-colon,       % │ unnötige Warnungen ausschalten
    mathtools-overbracket, % │
  },                       % ┘
]{unicode-math}

% traditionelle Fonts für Mathematik
\setmathfont{Latin Modern Math}
% Alternativ zum Beispiel:
%\setmathfont{Libertinus Math}

\setmathfont{XITS Math}[range={scr, bfscr}]
\setmathfont{XITS Math}[range={cal, bfcal}, StylisticSet=1]

% Zahlen und Einheiten
\usepackage[
  % locale=DE,                   % deutsche Einstellungen
  locale=US,
  separate-uncertainty=true,   % immer Fehler mit \pm
  per-mode=symbol-or-fraction, % / in inline math, fraction in display math
]{siunitx}

% chemische Formeln
\usepackage[
  version=4,
  math-greek=default, % ┐ mit unicode-math zusammenarbeiten
  text-greek=default, % ┘
]{mhchem}

% richtige Anführungszeichen
\usepackage[autostyle]{csquotes}

% schöne Brüche im Text
\usepackage{xfrac}

% Standardplatzierung für Floats einstellen
\usepackage{float}
\floatplacement{figure}{htbp}
\floatplacement{table}{htbp}

% Floats innerhalb einer Section halten
\usepackage[
  section, % Floats innerhalb der Section halten
  % below,   % unterhalb der Section aber auf der selben Seite ist ok
]{placeins}

% Seite drehen für breite Tabellen: landscape Umgebung
\usepackage{pdflscape}

% Captions schöner machen.
\usepackage[
  labelfont=bf,        % Tabelle x: Abbildung y: ist jetzt fett
  font=small,          % Schrift etwas kleiner als Dokument
  width=0.9\textwidth, % maximale Breite einer Caption schmaler
]{caption}
% subfigure, subtable, subref
\usepackage{subcaption}

% Grafiken können eingebunden werden
\usepackage{graphicx}
% größere Variation von Dateinamen möglich
\usepackage{grffile}

% schöne Tabellen
\usepackage{booktabs}

% Verbesserungen am Schriftbild
\usepackage{microtype}

% Grafiken können in LaTex gemalt werden
\usepackage{tikz, pgfplots}

% Ermöglicht relative Positionierung von tikz-Nodes
\usetikzlibrary{positioning}

% Für Feynman-Graphen mit Tikz
\usepackage{feynmp-auto}

% Für komplexere Captions
\usepackage{caption}

% Für mehrere Spalten
\usepackage{multicol}

% Literaturverzeichnis
\usepackage[
backend=biber,
sorting=none,
]{biblatex}
% Quellendatenbank
\addbibresource{lit.bib}
% \addbibresource{programme.bib}

% Hyperlinks im Dokument
\usepackage[
  german,
  unicode,        % Unicode in PDF-Attributen erlauben
  pdfusetitle,    % Titel, Autoren und Datum als PDF-Attribute
  pdfcreator={},  % ┐ PDF-Attribute säubern
  pdfproducer={}, % ┘
]{hyperref}
% erweiterte Bookmarks im PDF
\usepackage{bookmark}

% Trennung von Wörtern mit Strichen
\usepackage[shortcuts]{extdash}

% Tabellen und Text mögen sich jetzt
\usepackage{wrapfig}

\author{%
  Henry Krämerkämper\\%
  \href{mailto:henry.kraemerkaemper@tu-dortmund.de}{henry.kraemerkaemper@tu-dortmund.de}%
}
\publishers{TU Dortmund – Fakultät Physik}

\subject{Maschinelles Lernen für Physiker}
\title{The Genre Factor}
\date{\today}

% tu logo on top of the titlepage
\titlehead{\includegraphics[height=1.5cm]{figures/tu-logo.pdf}}

\begin{document}
\pagenumbering{roman}
\maketitle
\thispagestyle{empty}
\tableofcontents
\newpage
\pagenumbering{arabic}

\section{Introduction}
The task of classifying the genre of a song is common in the digital music industry. Most services
offering music listening present some information about each song, which often includes the genre.
Some services might even use the information to suggest other songs to listen to, which requires
accurate information about the genre (or the genres) that a song belongs to. Retrieving this
information is not easy, since there are no clear definitions of a genres attributes. Additionally,
most songs do not belong to only one genre. The genre itself might change over time as well, which
further complicates the problem. While the classification task might be technically solvable by humans,
it remains a non-trivial endeavour due to its inherent complexity. Given the immense size of most
music libraries, a manual approach to classification becomes highly impractical, necessitating alternative,
more efficient solutions. \\
\\
\noindent
With these factors in mind, the task is evidently predisposed to a solution via a machine learning approach.
As such, this strategy has become prevalent in addressing this problem, with a plethora of diverse methods having been explored to date
(see, for example~\cite{Übersicht2011}).
In this study, we attempt to classify music genres using a dense neural network. For this, we use a dataset sourced
from the website Kaggle~\cite{Kaggle} containing songs and their attributes taken from the services YouTube~\cite{Youtube}
and Spotify~\cite{Spotify}. We compare the neural network with two other, less sophisticated machine learning techniques,
namely support vector machines~\cite{SupportVector} and the $k$-nearest-neighbours-approach~\cite{NearestNeighbours},
to establish a baseline. We aim to find out whether employing more complex and labour-intensive techniques result in an
improvement in the face of the limited information contained in the dataset. \\
\\
\noindent
The report is structured as follows; first, the utilized dataset and the applied preprocessing is described in detail.
Subsequently, the architecture of the dense neural network is laid out and the results are presented. These findings are then
compared to the results of the alternative approaches. Finally, we draw a conclusion based on our analysis.
\section{The Utilized Dataset}
\subsection{Sourcing the Data}
The dataset used in this project contains $26$ attributes about $18862$ songs from $2079$ unique artists~\cite{Datensatz}.
However, the genre of the song is not included in the dataset; we query wikidata for the corresponding genre of each song,
using the python package \texttt{pywikibot}~\cite{pywikibot}. An example entry of the resulting dataset at this stage
is shown in table \ref{tab:attributes}. We do not keep all of these attributes; since the architecture of our
neural network does not feature text embedding, the attributes \texttt{Track}, \texttt{Album}, \texttt{Title}, \texttt{Channel} and
\texttt{Description} are dropped. The features \texttt{Uri} and \texttt{Url\_youtube} are most likely random and do not contain
useful information for our models to learn, therefore these are not used as well.
\FloatBarrier
\begin{table}[H]
  % \centering
  \footnotesize
  \begin{tabular}{l l l l}
    \toprule
    $\text{Feature}$ & $\text{Example}$ & $\text{Feature}$ & $\text{Example}$ \\
    \midrule
    Artist &  Gorillaz &                                  Valence & 0.772 \\
    Url\_spotify & \url{https://open.spotify...} &        Tempo & 138.559 \\
    Track & Feel Good Inc. &                              Duration\_ms & 222640.0 \\
    Album & Demon Days   &                                Url\_youtube & \url{https://www.youtube...} \\
    Album\_type & album  &                                Title & Gorillaz - Feel Good Inc. (Official... \\
    Uri & spotify:track:0d28khcov6AiegS...  &             Channel & Gorillaz \\
    Danceability & 0.818 &                                Views & 693555221.0 \\
    Energy & 0.705 &                                      Likes & 6220896.0 \\
    Key & 6.0 &                                           Comments & 169907.0 \\
    Loudness & -6.679  &                                  Description & Official HD Video for Gorillaz'... \\
    Speechiness & 0.177  &                                Licensed & True \\
    Acousticness & 0.00836  &                             official\_video & True \\
    Instrumentalness & 0.00233 &                          Stream & 1040234854.0 \\
    Liveness & 0.613  &                                   Genre & Hip Hop \\
    \bottomrule
  \end{tabular}
  \normalsize
  \caption{The attributes contained in the dataset, shown for an example song.}
  \label{tab:attributes}
\end{table}
\FloatBarrier
\subsection{Preprocessing}
Subsequently, the dataset is cleaned; missing or erroneous values
in numerical features are substituted by a value derived by a $k$-nearest-neighbours-approach. We implement this
by using the \texttt{SimpleImputer} from the \texttt{Scikit-Learn} package~\cite{scikit-learn}.
The cardinal features are then scaled to a range of $[-1,1]$ and transformed to follow a normal distribution to improve numerical stability
and the convergence speed as well as preventing the 'Exploding/Vanishing-Gradient' problem~\cite{geron}. These steps are implemented using \texttt{QuantileTransformer} and
\texttt{MinMaxScaler} from \texttt{Scikit-Learn}~\cite{scikit-learn}.
The aforementioned wikidata query results in $397$ different genres. As these are highly specific, these categories are consolidated into $26$ broader genres to
achieve a more streamlined dataset and increase the sample size per class. For example, the genre 'latin' contains 'salsa', 'bossa nova', 'samba' among others.
Given the constraints of our datasets size, we only keep $6$ genres to further increase the sample
size. The remaining genres are hip hop, rock, pop, electronic, metal and classic. The datasets class imbalance can be seen in figure \ref{fig:class-imbalance}.
\FloatBarrier
\begin{figure}[h]
  \centering
  \includegraphics[scale=0.6]{figures/genre_hist_oranges.pdf}
  \caption{Class imbalance of the resulting dataset.}
  \label{fig:class-imbalance}
\end{figure}
\FloatBarrier
\noindent
Upon application of the preprocessing steps, the dataset is reduced to $6446$ entries.
Given the high class imbalance in our dataset, the use of weights could potentially help mitigate this imbalance. However, in our specific case, we found that weighting did not improve model performance. Therefore, we decided not to use weighting in our final modelling approach.
After this step, the dataset is divided into a training-, test- and validation-set, with each containing $\SI{49}{\percent}$, $\SI{30}{\percent}$ and $\SI{21}{\percent}$
of the data respectively. In this process, we use stratified sampling to ensure that the distribution of classes in each subset mirror the distribution in the original
set.
\section{The Neural Network Model}
\subsection{Architecture and Implementation}
The sequential dense neural network employed to solve this task is composed of two hidden layers. Increasing the layer count further proved to result in no additional
performance gains. The same holds true for using more neurons per layer, suggesting both strategies may lead to over-fitting.
To combat over-fitting, we use dropouts between each dense layer as well as learning rate decay and early stopping.
The later also reduces the needed computation time and annealing the learning rate increases the convergence speed. The output layer uses the activation function softmax,
as it can be used to return a probability distribution for the classes. To measure the performance and loss, we use the accuracy and the categorical crossentropy respectively.
The network architecture and the output shape of each layer can be seen in figure~\ref{fig:nn_layers}.
\begin{figure}[H]
    \centering
    \begin{minipage}{0.49\textwidth}
        \centering
        \small
        \begin{tabular}{l c}
            \toprule
            Parameter & Value \\
            \midrule
            Activation Function     & LeakyReLU \\
            Batch Size              & $512$     \\
            Dropout Rate            & $0.4$     \\
            Early Stopping Patience & $20$      \\
            Epochs                  & $60$      \\
            Neurons Layer (1)       & $256$     \\
            Neurons Layer (2)       & $512$     \\
            Neurons Layer (3)       & $512$     \\
            Number of hidden Layers & $2$       \\
            Optimizer               & Nadam     \\
            Reduce LR Factor        & $0.1$     \\
            Min. LR Rate            & $0.0$     \\
            Reduce LR Patience      & $5$       \\
            Output Function         & Softmax   \\
            % Weight Constraint       & $10$      \\
            \bottomrule
        \end{tabular}
        % \caption{All parameters of the best performing model.}
        \label{tab:my_label_for_table}
    \end{minipage}
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=0.5\textwidth]{figures/Network.pdf}
        % \caption{Overview of the neural networks' layers and parameters.}
        % \label{fig:nn_layers}
        \normalsize
    \end{minipage}\hfill
  \caption{Overview of the neural networks' layers and parameters.}
  \label{fig:nn_layers}
\end{figure}
\noindent
We implement the model using \texttt{tensorflow}~\cite{tensorflow} and \texttt{keras}~\cite{keras}.
\subsection{Hyperparameter Optimization}
The exact parameters given in figure~\ref{fig:nn_layers} are the result of performing a hyperparameter optimization. For this, we carry out a
grid search~\cite{gridsearch}, implemented with \texttt{GridSearchCV} from \texttt{Scikit-Learn}~\cite{scikit-learn}, as it supports parallel computing.
We use $3$-fold cross-validation  to mitigate overfitting and to make efficient use of the finite data.
To be able to optimize a model created by \texttt{keras} with \texttt{Scikit-Learn} functions, we use \texttt{SciKeras}~\cite{scikeras}. As we were limited in time and
computational resources, only a subset of the parameter space has been explored; for most parameters, we tried between two and six values, but
not all combinations of them. Still, $2033$ models were tested, with \ref{fig:nn_layers} resulting in the highest performance. All tested models and their performance can be
seen here\footnote{\url{https://github.com/Henry-Kr15/TheGenreFactor/blob/main/HPO-data/HPO_combined.csv}}.
Selecting the best model is a trade-off between accuracy on the training set and overfitting, which results in lower performance on the validation set. Because of the limited
size of the validation set (as it comprises of only $\SI{21}{\percent}$ of the data), relying on validation accuracy alone could prove inconsistent. We try to manually alter
this trade-off to achieve higher generalisation than otherwise possible. For this, we combine the validation accuracy and training accuracy into a new metric which we call
delta accuracy by taking the difference of the two. Then we calculate a score based on the delta and validation accuracy as follows:
\begin{equation}
	\text{score} = \frac{1}{2} \cdot (w_{1} \cdot (1 - \text{acc}_{\text{delta}}) + w_{2} \cdot \text{acc}_{\text{val}}).
  \label{eqn:score}
\end{equation}
The score minimizes $\text{acc}_{\text{delta}}$ whilst maximizing $\text{acc}_{\text{val}}$. Using the weights $w_{1}$ and $w_{2}$, the importance of each value can be specified.
All models can be seen in figure~\ref{fig:HPO_scatter}. With $w_{1}=0.5$ and $w_{2}=1$ the model marked red posses the best score.
\begin{figure}[H]
  \centering
  \includegraphics[scale=0.70]{figures/NN/HPO_scatter.pdf}
  \caption{Comparison of training and delta accuracy for all models.}
  \label{fig:HPO_scatter}
\end{figure}
\subsection{Performance and Results}
The model achieves $\SI{64.43}{\percent}$ accuracy on the test data set. Accuracy and loss are shown in figure~\ref{fig:nn_acc} and figure~\ref{fig:nn_loss} respectively.
The accuracy is the proportion of correct predictions made by a model out of all predictions, while loss is a measure of how well a model's predictions match the actual
values, with lower loss indicating better performance.
\FloatBarrier
\begin{figure}[H]
  \centering
  \includegraphics[scale=0.7]{figures/NN/Acc.pdf}
  \caption{Plot of the accuracy of the neural network on the training and validation dataset.}
  \label{fig:nn_acc}
\end{figure}
\begin{figure}[ht]
  \centering
  \includegraphics[scale=0.7]{figures/NN/Loss.pdf}
  \caption{Plot of the loss curve of the neural network on the training and validation dataset.}
  \label{fig:nn_loss}
\end{figure}
\FloatBarrier
\noindent
The accuracy curve shows a satisfactory trend, indicating that the model is performing well on both the training and validation datasets. The loss curve however
shows a slight indication of overfitting after approximately $40$ epochs.
The confusion matrix~\cite{geron} can be seen in figure~\ref{fig:nn_confusion}.
\begin{figure}[H]
  \centering
  \includegraphics[scale=0.33]{figures/NN/confusion_matrix_nn.png}
  \caption{Confusion matrix of the neural network on the test dataset.}
  \label{fig:nn_confusion}
\end{figure}
\noindent
The confusion matrix shows, that although the models performance is for the most part adequate, problems arise for genres which consist of a broader spectrum with less clear
defined characteristics, such as pop or electronic.
The precision-recall curve~\cite{geron}, depicted in figure~\ref{fig:pr_curve_nn}, shows a comparable result.
\begin{figure}[H]
  \centering
  \includegraphics[scale=0.7]{figures/NN/PR_curve_genres.pdf}
  \caption{Precision-recall curve for each genre on the test dataset.}
  \label{fig:pr_curve_nn}
\end{figure}
\noindent
The figures display the precision-recall area under curve (PR-AUC) as well; it is a performance measure for classification models at various threshold levels. It quantifies
how well the model manages True Positives and False Positives across all thresholds. A PR-AUC value close to $1$ indicates a model with high precision and recall, while a
value close to $0$ suggests poor performance~\cite{geron}. It can be seen that the PR-AUC differs significantly between the genres, supporting previous observations.
Astonishingly, the neural network performs well for classic, which has the lowest sample size of any genre in the dataset.
Classic and hip hop seem to get differentiated well, while pop and electronic show suboptimal performance.

\section{Alternative Approaches to the Problem}
\subsection{Exploration of alternative Methods: K-Nearest Neighbours}
In addition to the neural network, we also explored other machine learning methods for comparison. One such method is the K-Nearest Neighbours (KNN) algorithm, a simpler
method which can be used for classification as well. This section provides an overview of our implementation and evaluation of the KNN approach.
The \texttt{Scikit-Learn}-package~\cite{scikit-learn} provides an implementation of a Knn algorithm, the \texttt{KNeighborsClassifier}.
We use the $12$ nearest neighbours and the euclidean metric for the classification, as this results in the highest performance. The model achieves an accuracy of $\SI{58.48}{\percent}$ on the test
dataset.
The resulting precision-recall curves for each genre are depicted in figure~\ref{fig:pr_knn}.
\begin{figure}[H]
  \centering
  \includegraphics[scale=0.7]{figures/knn/PR_KNN.pdf}
  \caption{Precision-Recall curves of the Knn-algorithm for all genres.}
  \label{fig:pr_knn}
\end{figure}
\noindent
The precision-recall curves show the limitations of the Knn-algorithm regarding more complex problems as the curves are significantly worse than the ones
from the neural network. Although the accuracy is not far behind compared to the neural network (as they are in $\num{10}$ percentage points range of each other) the
differences in handling complexity between the models become apparent.
The confusion matrix, depicted in figure~\ref{fig:conf_knn}, further highlights these differences.
\begin{figure}[H]
  \centering
  \includegraphics[scale=0.33]{figures/knn/confusion_matrix_knn.png}
  \caption{Confusion matrix of the Knn-algorithm on the test dataset.}
  \label{fig:conf_knn}
\end{figure}
\noindent
The values on the diagonal show adequate performance, but the spread is substantially higher compared to the neural network.

\subsection{Exploration of alternative Methods: Support Vector Machines}
Another alternative method we investigated is the Support Vector Machine (SVM) approach. Through combining multiple SVMs, using them for classification tasks with multiple
classes is possible.
This section discusses our implementation and performance of the SVM method in the context of our project.
In standard form, support vector machines can only be used for binary classification. Here, we use an 'One-vs-One' approach: for every pair of genres, one support vector
machine is trained. The multiclass classification is then performed by majority vote of the SVMs. The \texttt{Skicit-Learn}-package~\cite{scikit-learn} provides the \texttt{svm.SVC} method,
which we use with the radial basis function (RBF) as kernel. This model achieves an accuracy of $\SI{62.25}{\percent}$ on the test dataset. The resulting
precision-recall curves for each class are depicted in figure~\ref{fig:svm_pr}.
\begin{figure}[H]
  \centering
  \includegraphics[scale=0.7]{figures/svm/PR_SVM.pdf}
  \caption{Precision-recall curves for all classes of the SVMs on the test dataset.}
  \label{fig:svm_pr}
\end{figure}
\noindent
The precision-recall curves display a better performance than the Knn-algorithm but are inferior to the neural network, which is also resembled in their differences
in their accuracy score. The confusion matrix is depicted in figure~\ref{fig:conf_svm}.
\begin{figure}[H]
  \centering
  \includegraphics[scale=0.33]{figures/svm/confusion_matrix_svm.png}
  \caption{Confusion matrix of the SVMs on the test dataset.}
  \label{fig:conf_svm}
\end{figure}
\noindent
The performance is only marginally inferior to that of the neural network, a surprising outcome considering the significantly shorter time required to implement and optimize
the support vector machines compared to the neural network.

\section{Discussion and Insights}
Regarding the task of classifying a songs genre, all strategies implemented in this project display only adequate performance. Figure~\ref{fig:performance_comp}
summarizes the accuracies of the three tested methods and compares them to completely random guessing and the strategy of always guessing the most frequently occurring
genre.
\begin{figure}[H]
  \centering
  \includegraphics[scale=0.25]{figures/performance-comparison.pdf}
  \caption{Comparison of the accuracy of different approaches.}
  \label{fig:performance_comp}
\end{figure}
\noindent
As model complexity increases, we observe only diminishing returns. Still, all three machine learning algorithms improve significantly upon the guessing
methods. This suggests that we may be  limited by the information contained in the dataset; it appears that no further differentiation between the different genres is
possible based on the available features. This is reflected by the results of the extensive hyperparameter optimization, which yielded no significant improvements.
These limitations could also be influenced by several factors in the preprocessing steps. For instance, there is no way of actually verifying the wikidata query
beyond checking random samples.
The grouping of sub-genres into larger, more general genres may also dilute the datasets informational value, as genre differentiation becomes less distinct.
Furthermore, since genres are not clearly defined to begin with, this ambiguity could pose a fundamental challenge.
The class imbalace may also contribute to difficulties with the classification of smaller genres.
To enhance the sample size for the under-represented classes, synthesizing new entries could be a potential strategy for future work.
The attributes used by the model such as \texttt{Speechiness}, \texttt{Danceability} and others may also be too abstract as they represent high level descriptions
of the actual songs. An architecture that accepts complete sound files as input and utilizes lower level attributes could possibly improve the classification
performance significantly~\cite{Übersicht2011}. This hypothesis is supported by the fact that music genres like classic and hip hop, which possess distinct
characteristics that might be reflected in high-level attributes, perform better across all three models.
The limited size of the dataset used might negatively impact the models capabilities as well. This constraint also necessitated our approach to selecting the best model while
mitigating overfitting by using an arbitrarily weighted score. It is important to note that this approach introduces a degree of subjectivity and uncertainty. The choice of
weights can significantly influence the final score, and different weight assignments might lead to different conclusions. Therefore, the results obtained using this weighted
score should be interpreted with caution and further validated with more robust methods.
However, most of these challenges should be negligible for a music service provider, given their access to large datasets and sufficient resources to produce accurate and
validated truth values. In such a scenario, a neural network should be capable of learning enough information to significantly improve the classification accuracy. From a
runtime and resource usage perspective, the computationally more demanding process of training and optimizing a neural network becomes viable when compared to the simpler
machine learning methods explored in this project.

\newpage
\printbibliography

\end{document}
